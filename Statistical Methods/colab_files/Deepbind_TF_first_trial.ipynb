{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deepbind TF - first trial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uosH3ughmaVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a8c690f-afd8-4509-a664-93e84807933a"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'02 Graphs and Chart in R-2012.pdf'\n",
            "'12th marksheet.jpg'\n",
            " 20180823_095003-1.jpg\n",
            " 20201011_112101222.pdf\n",
            " 20eed8a3ceee4222a33f60d500d07668-0001.jpg\n",
            "'3452_5 (1).pdf'\n",
            "'3452_5 (2).pdf'\n",
            "'3452_5 (3).pdf'\n",
            " 3452_5.pdf\n",
            "'3456_Radhika Sethi_Photo (1).jpeg'\n",
            "'3456_Radhika Sethi_Photo (2).jpeg'\n",
            "'3456_Radhika Sethi_Photo.jpeg'\n",
            "'3456_Radhika Sethi_Resume (1).pdf'\n",
            "'3456_Radhika Sethi_Resume (2).pdf'\n",
            "'3456_Radhika Sethi_Resume.pdf'\n",
            "'3947_Assignment 1.gdoc'\n",
            "'3947_Assignment 2.gdoc'\n",
            "'3947_Assignment 3.gdoc'\n",
            "'3947_Assignment 4.gdoc'\n",
            "'3947_Assignment 5 .gdoc'\n",
            "'3947_Assignment 6 .gdoc'\n",
            "'3947_Assignment 7.gdoc'\n",
            "'3947_Assignment 8.gdoc'\n",
            "'3947_DMDW Assignment 1.gdoc'\n",
            "'3947_DMDW Assignment 2.gdoc'\n",
            "'3947_DMDW Assignment 3 - Weka.gdoc'\n",
            "'3947_DMDW Assignment 4 - Weka.gdoc'\n",
            "'3947_DMDW Group C Assignment .gdoc'\n",
            " 3947_Hotel_Management.pdf\n",
            "'3947_IOT_Assignment 1.gdoc'\n",
            "'3947_PSDL Python Assignment 2.gdoc'\n",
            "'3947_PSDL Python Assignment 3.gdoc'\n",
            "'3947_PSDL Python Assignment 4.gdoc'\n",
            "'3947_PSDL Python Assignment 5.gdoc'\n",
            "'3947_Shreya Pawaskar_Resume (1).pdf'\n",
            "'3947_Shreya Pawaskar_Resume (2).pdf'\n",
            "'3947_Shreya Pawaskar_Resume.gdoc'\n",
            "'3947_Shreya Pawaskar_Resume Part B.gdoc'\n",
            "'3947_Shreya Pawaskar_Resume.pdf'\n",
            "'3964_Aanchal Tulsiani_Photo (1).jpeg'\n",
            "'3964_Aanchal Tulsiani_Photo (2).jpeg'\n",
            "'3964_Aanchal Tulsiani_Photo.jpeg'\n",
            "'3964_Aanchal Tulsiani_Resume.pdf'\n",
            " 3adf76aa280148739a29dc75121d52f2-0001.jpg\n",
            "'4924_ Assignment 6.pdf'\n",
            " 4947_internship_report.gdoc\n",
            "'4947_IOT_Assignment 2.gdoc'\n",
            "'4947_IOT_Assignment 3.gdoc'\n",
            "'4947_IOT_Assignment 4.gdoc'\n",
            "'4947_IOT_Assignment 5.gdoc'\n",
            "'4947_IOT_Assignment 6.gdoc'\n",
            " Aanchal_Tulsiani_PartA.doc\n",
            " Aanchal_Tulsiani_PartA.gdoc\n",
            " Aanchal_Tulsiani_PartB.doc\n",
            " abcd.java\n",
            "'Abstract Draft - Shreya Pawaskar.gdoc'\n",
            "'AICVS Website.zip'\n",
            "'Amex AI ML Hackathon- Geek Goddess 2021 PPT.gslides'\n",
            " Assignment1\n",
            "'Assignment 3_3947.gdoc'\n",
            "'Assignment 7_3947.gdoc'\n",
            " Assignment8\n",
            "'Assignment 8_3947.gdoc'\n",
            "'AY 2020-21 ALL T2 SEM-II_PECE3202A.xls'\n",
            " Background+Check+Authorization+V3.gdoc\n",
            " Background+Check+Authorization+V3.pdf\n",
            " best_hyperpamarameters.pth\n",
            "'Bhumi-collar making materials.gslides'\n",
            "'BHUMI PAWS - collar making.gslides'\n",
            "'BHUMI PAWS - Pune Chapter.gslides'\n",
            "'BHUMI PAWS - Pune Chapter.pptx'\n",
            "'Blog_Anaya (1).docx'\n",
            " Blog_Anaya.docx\n",
            " Books.zip\n",
            "'C22018881961-3947-Shreya Pawaskar-assignment1.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-assignment2.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 3.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 4.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 5.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 6.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 7.gdoc'\n",
            "'C22018881961-3947-Shreya Pawaskar-Assignment 8.gdoc'\n",
            "'career launcher letter (1).pdf'\n",
            "'career launcher letter.pdf'\n",
            " Certificate\n",
            " Classroom\n",
            "'Cloud Counselage - Appointment Letter.pdf'\n",
            "'Cloud Counselage – Internship Experience Letter - Shreya Pawaskar (1).pdf'\n",
            "'Cloud Counselage – Internship Experience Letter - Shreya Pawaskar.gdoc'\n",
            "'Cloud Counselage – Internship Experience Letter - Shreya Pawaskar.pdf'\n",
            "'Cloud Counselage- Visualization.gdoc'\n",
            " Code.pdf\n",
            "'Colab Notebooks'\n",
            "'Company List.gsheet'\n",
            "'Copy of INTERNSHIP PROCESS EXPERIENCE RECORD.gsheet'\n",
            "'cummins id card both side.pdf'\n",
            "'Div I.pdf'\n",
            " dms.gdoc\n",
            " exercise1.R\n",
            " Exercise2.R\n",
            "'Explore ML Certificate.pdf'\n",
            "'FINAL PPT (3).pptx'\n",
            "'Foundational-FinalPPTTemplate_MILESTONE 4-July152020 (1).pptx'\n",
            "'Foundational-FinalPPTTemplate_MILESTONE 4-July152020 (2).pptx'\n",
            "'Foundational-FinalPPTTemplate_MILESTONE 4-July152020.gslides'\n",
            "'Foundational-FinalPPTTemplate_MILESTONE 4-July152020.pptx'\n",
            " GoldmanSachs-Sop-Shreya.gdoc\n",
            "'Group 6_ Abstract.gdoc'\n",
            "'Group 6_ Abstract.pdf'\n",
            " Group6_undertaking.docx\n",
            "'Group 6_undertaking.pdf'\n",
            "'GSSOC_NDA (1).pdf'\n",
            " GSSOC_NDA.pdf\n",
            "'IIE Application-8-10.pdf'\n",
            "'internship completion template (4).gdoc'\n",
            "'internship completion template (4).pdf'\n",
            "'introduction and Team info .gslides'\n",
            "'kcg-vjor-pef (2021-04-28 at 20_36 GMT-7).mp4'\n",
            "'lean canvas-converted.pdf'\n",
            "'Lecture1-ML (AIML)Sent.gdoc'\n",
            "'Lessons learned (6).gsheet'\n",
            "'Meet Recordings'\n",
            " MyModel_2.pth\n",
            " Presentation-Sample-AIML-Hackathon.gslides\n",
            " professearch\n",
            "'PROJECT CHARTER  (1).xlsx'\n",
            "'PROJECT CHARTER .gsheet'\n",
            "'PROJECT CHARTER .xlsx'\n",
            " ProjectCharter.xlsx.gsheet\n",
            "'Project Plan (4) (1).gsheet'\n",
            "'Project Plan (4) (1).xlsx'\n",
            "'Project Plan (4) (2).gsheet'\n",
            "'Project Plan (4) (2).xlsx'\n",
            "'Project Plan (4).gsheet'\n",
            "'Project Plan (4).xlsx'\n",
            "'Prototyping presentation.gslides'\n",
            " PV_Milestone_PPT_3-June302020.pptx\n",
            " q1.docx\n",
            " q2b.docx\n",
            " Relief-suc_02.03.2021.xlsx\n",
            "'Research Project Email.gdoc'\n",
            " Result_sheet_ALL_gradereport.pdf\n",
            "'Resume Format1.gdoc'\n",
            "'Resume Format.doc'\n",
            "'resume format_Personal Details Part A.doc'\n",
            "'Resume format Personal Details Part B.doc'\n",
            "'Resume Shreya (1).pdf'\n",
            " Resume_Shreya_Pawaskar.gdoc\n",
            "'Resume Shreya.pdf'\n",
            " Scanned.pdf\n",
            "'Screenshot (229).png'\n",
            "'Seminar Notes.gdoc'\n",
            "'Shreya Pawaskar - AmEx Hackathon Report.gdoc'\n",
            "'Shreya Pawaskar_CLAP_Certificate.pdf'\n",
            "'Shreya Pawaskar Marks.gdoc'\n",
            "'shreya pawaskar resume.pdf'\n",
            " Shreya_Pawaskar_Resume.pdf\n",
            " Shreya_Pawaskar_SecondYearMarksheet-min.pdf\n",
            "'Shreya Pawaskar_Seminar PPT_3947.gslides'\n",
            "'Shreya Pawaskar_Seminar Report_3947.gdoc'\n",
            "'shreya resume1.pdf'\n",
            " shreya_resume.pdf\n",
            "'shreya second year fee receipt.pdf'\n",
            " SIH_DATASET_UPDATED.csv\n",
            "'SRS .gsheet'\n",
            "'SRS .xlsx'\n",
            " test.R\n",
            "'Third year marks screenshot'\n",
            " tut6.pdf\n",
            " undertaking.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:40:57.598540Z",
          "start_time": "2021-06-01T12:40:57.495684Z"
        },
        "id": "0Dkq9-BiWo1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c9752e-da48-497d-dfab-986d7a8fcdc4"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:40:55.967483Z",
          "start_time": "2021-06-01T12:40:53.993414Z"
        },
        "id": "UiSAAZ_OkWri"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import csv\n",
        "import math \n",
        "import random\n",
        "import gzip\n",
        "from scipy.stats import bernoulli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:07.326068Z",
          "start_time": "2021-06-01T12:41:07.323894Z"
        },
        "id": "jASRrXw4kWrn"
      },
      "source": [
        "nummotif=16 #number of motifs to discover\n",
        "bases='ACGT' #DNA bases\n",
        "basesRNA='ACGU'#RNA bases\n",
        "batch_size=64 #fixed batch size -> see notes to problem about it\n",
        "dictReverse={'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:09.726728Z",
          "start_time": "2021-06-01T12:41:09.724519Z"
        },
        "id": "iWEt7WVVkWrp"
      },
      "source": [
        "class Experiment:\n",
        "    def __init__(self,filename,motiflen):\n",
        "        self.file=filename\n",
        "        self.motiflen=motiflen\n",
        "    \n",
        "    def getMotifLen(self):\n",
        "        return self.motiflen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:20.384069Z",
          "start_time": "2021-06-01T12:41:20.380294Z"
        },
        "id": "Jhp-yVajkWrs"
      },
      "source": [
        "def seqtopad(sequence,motlen,kind='DNA'):\n",
        "    rows=len(sequence)+2*motlen-2\n",
        "    S=np.empty([rows,4])\n",
        "    base= bases if kind=='DNA' else basesRNA\n",
        "    for i in range(rows):\n",
        "        for j in range(4):\n",
        "            if i-motlen+1<len(sequence) and sequence[i-motlen+1]=='N' or i<motlen-1 or i>len(sequence)+motlen-2:\n",
        "                S[i,j]=np.float32(0.25)\n",
        "            elif sequence[i-motlen+1]==base[j]:\n",
        "                S[i,j]=np.float32(1)\n",
        "            else:\n",
        "                S[i,j]=np.float32(0)\n",
        "    return S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:21.556510Z",
          "start_time": "2021-06-01T12:41:21.554166Z"
        },
        "id": "qujC_-HAkWru"
      },
      "source": [
        "def dinucshuffle(sequence):\n",
        "    b=[sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
        "    random.shuffle(b)\n",
        "    d=''.join([str(x) for x in b])\n",
        "    return d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:22.943707Z",
          "start_time": "2021-06-01T12:41:22.941286Z"
        },
        "id": "ju_lj4g8kWrw"
      },
      "source": [
        "def logsampler(a,b):\n",
        "    x=tf.Variable(tf.random_uniform([],minval=0,maxval=1), trainable=False)\n",
        "    y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
        "    \n",
        "#     x=np.random.uniform(low=0,high=1)\n",
        "#     y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:24.171140Z",
          "start_time": "2021-06-01T12:41:24.168923Z"
        },
        "id": "MFBv7OV-kWrz"
      },
      "source": [
        "def sqrtsampler(a,b):\n",
        "    x=tf.Variable(tf.random_uniform([],minval=0,maxval=1), trainable=False)\n",
        "#     x=np.random.uniform(low=0,high=1)\n",
        "    y=(b-a)*(x**0.5)+a\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:25.652610Z",
          "start_time": "2021-06-01T12:41:25.648765Z"
        },
        "id": "5NkULJwDkWr1"
      },
      "source": [
        "class Chip(Experiment):\n",
        "    def __init__(self,filename,motiflen=24):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "            \n",
        "    def openFile(self):\n",
        "        train_dataset=[]\n",
        "     \n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            \n",
        "            for row in reader:\n",
        "                    train_dataset.append([seqtopad(row[2],self.motiflen),[1]])\n",
        "                    \n",
        "                    train_dataset.append([seqtopad(dinucshuffle(row[2]),self.motiflen),[0]])\n",
        "                   \n",
        "        \n",
        "        \n",
        "        random.shuffle(train_dataset)\n",
        "        frac1=int(len(train_dataset)*1/3)\n",
        "        frac2=int(len(train_dataset)*2/3)\n",
        "        return train_dataset[:frac1],train_dataset[frac1:frac2],train_dataset[frac2:],train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:27.609542Z",
          "start_time": "2021-06-01T12:41:27.607943Z"
        },
        "id": "a48AQrhKkWr3"
      },
      "source": [
        "filename='/content/sampledataset1.seq.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:41:37.684824Z",
          "start_time": "2021-06-01T12:41:30.474707Z"
        },
        "id": "tByxbfE2kWr6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "5f4b6893-b1bf-47ec-db03-49dfb8a78a23"
      },
      "source": [
        "test= Chip(filename)\n",
        "d1,d2,d3,dataAll =test.openFile()\n",
        "\n",
        "data1=np.asarray([el[0] for el in d1],dtype=np.float32)\n",
        "label1=np.asarray([el[1] for el in d1],dtype=np.float32).reshape(len(data1),1)\n",
        "\n",
        "data2=np.asarray([el[0] for el in d2],dtype=np.float32)\n",
        "label2=np.asarray([el[1] for el in d2],dtype=np.float32).reshape(len(data2),1)\n",
        "\n",
        "data3=np.asarray([el[0] for el in d3],dtype=np.float32)\n",
        "label3=np.asarray([el[1] for el in d3],dtype=np.float32).reshape(len(data3),1)\n",
        "\n",
        "data=[data1,data2,data3]\n",
        "label=[label1,label2,label3]\n",
        "\n",
        "data_all=np.asarray([el[0] for el in dataAll],dtype=np.float32)\n",
        "label_all=np.asarray([el[1] for el in dataAll],dtype=np.float32).reshape(len(data_all),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-80e4d3c92493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mChip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataAll\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d65d182120aa>\u001b[0m in \u001b[0;36mopenFile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseqtopad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotiflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseqtopad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdinucshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotiflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6dca7239beb8>\u001b[0m in \u001b[0;36mseqtopad\u001b[0;34m(sequence, motlen, kind)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmotlen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmotlen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'N'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmotlen\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmotlen\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmotlen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:44:33.791615Z",
          "start_time": "2021-06-01T12:44:33.785611Z"
        },
        "id": "8Jx0FGiJkWr8"
      },
      "source": [
        "def convolution(input_data, num_input_channels, num_filters, filter_shape, conv_weights,bias_weights,wd1,bd1,W,b,pooling,neuType,training,dropprob):\n",
        "\n",
        "    \n",
        "    # setup the convolutional layer operation\n",
        "    out_layer = tf.nn.conv1d(input_data, conv_weights, 1, padding='VALID')\n",
        "\n",
        "    out_layer= tf.subtract(out_layer,conv_bias)\n",
        "\n",
        "    # apply a ReLU non-linear activation\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "\n",
        "    # now perform pooling\n",
        "    if pooling == 'max_pool':\n",
        "        pool=tf.reduce_max(out_layer,axis=1) \n",
        "        \n",
        "    elif pooling == 'avg_pool':\n",
        "        out_layer1= tf.reduce_max(out_layer, axis=1)\n",
        "        out_layer2= tf.reduce_mean(out_layer, axis=1)\n",
        "        \n",
        "        x_expanded = tf.expand_dims(out_layer1, 2)                 \n",
        "        y_expanded = tf.expand_dims(out_layer2, 2)  \n",
        "        \n",
        "        concatted = tf.concat([x_expanded, y_expanded], 2)  \n",
        "\n",
        "        pool = tf.reshape(concatted, [-1, 2*num_filters]) \n",
        "        \n",
        "\n",
        "    t =tf.constant(1 ,dtype=tf.float32)\n",
        "    \n",
        "    def ifTrain(pool):\n",
        "        pooldrop = tf.nn.dropout(pool,keep_prob=dropprob)\n",
        "#         pooldrop=tf.multiply(pool,mask) \n",
        "        out = tf.matmul(pooldrop, wd1) + bd1\n",
        "        \n",
        "        return out\n",
        "    def ifTest(pool):\n",
        "        out = dropprob*tf.matmul(pool, wd1) + bd1\n",
        "        return out\n",
        "    \n",
        "    #check if there's hidden stage\n",
        "    if(neuType=='nohidden'):\n",
        "\n",
        "        out = tf.cond(tf.equal(training,t), lambda: ifTrain(pool), lambda: ifTest(pool))\n",
        "\n",
        "        \n",
        "    elif(neuType=='hidden'):\n",
        "\n",
        "\n",
        "        dense_layer1 = tf.matmul(pool, W) + b\n",
        "        dense_layer1=tf.nn.relu(dense_layer1)\n",
        "        \n",
        "        out = tf.cond(tf.equal(training,t), lambda: ifTrain(dense_layer1), lambda: ifTest(dense_layer1))\n",
        "        \n",
        "\n",
        "    return out\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T12:47:27.299820Z",
          "start_time": "2021-06-01T12:47:26.977702Z"
        },
        "id": "_6AXbP8EkWsA"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "graph=tf.Graph()\n",
        "with graph.as_default():\n",
        "    \n",
        "    num_input_channels=4\n",
        "    num_filters=16\n",
        "    filter_shape=24\n",
        "    pooling='avg_pool'\n",
        "    neuType='nohidden'\n",
        "    \n",
        "    beta1=tf.placeholder_with_default(logsampler(10**-15,10**-3),shape=())\n",
        "    beta2=tf.placeholder_with_default(logsampler(10**-10,10**-3),shape=())\n",
        "    beta3=tf.placeholder_with_default(logsampler(10**-10,10**-3),shape=())\n",
        "\n",
        "    \n",
        "    \n",
        "    learning_rate= tf.placeholder_with_default(logsampler(0.0005, 0.05),shape=())\n",
        "    momentum_rate= tf.placeholder_with_default(sqrtsampler(0.95, 0.99),shape=())\n",
        "    \n",
        "\n",
        "    batch_size=64\n",
        "    with tf.device('/gpu:0'):\n",
        "    \n",
        "      x = tf.placeholder(tf.float32, [None, 147, 4])\n",
        "      y = tf.placeholder(tf.float32,[None,1])\n",
        "      dropprob = tf.placeholder_with_default(1.0, shape=())\n",
        "\n",
        "      # Distinguish training and testing: training=1 for training , =0 for testing\n",
        "      training = tf.placeholder_with_default(0.0, shape=())\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "      \n",
        "      #Set up iterator for the data\n",
        "      dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "      dataset = dataset.shuffle(500).repeat().batch(batch_size)\n",
        "      \n",
        "                  \n",
        "      iterator = dataset.make_initializable_iterator()\n",
        "      data_X, data_y = iterator.get_next()      \n",
        "      data_y = tf.cast(data_y, tf.float32)\n",
        "      \n",
        "      \n",
        "   \n",
        "    with tf.device('/gpu:0'):\n",
        "      \n",
        "      conv_filt_shape = [filter_shape, num_input_channels, num_filters]\n",
        "\n",
        "      stdConv=tf.placeholder_with_default(logsampler(10**-7,10**-3),shape=()) \n",
        "      # initialise weights and bias for the filter\n",
        "      conv_weights = tf.Variable(tf.truncated_normal(conv_filt_shape, mean=0,stddev=stdConv), name='Conv1_W')\n",
        "      conv_bias = tf.Variable(tf.truncated_normal([num_filters]), name='Conv1_b')\n",
        "\n",
        "\n",
        "\n",
        "      if pooling=='max_pool':\n",
        "          W = tf.Variable(tf.truncated_normal([16,32], mean=0, stddev=0.3), name='W')\n",
        "          b = tf.Variable(tf.truncated_normal([32], mean=0, stddev=0.3), name='b')\n",
        "      else:\n",
        "          W = tf.Variable(tf.truncated_normal([32,32], mean=0, stddev=0.3), name='W')\n",
        "          b = tf.Variable(tf.truncated_normal([32], mean=0, stddev=0.3), name='b')     \n",
        "\n",
        "      if neuType == 'nohidden':\n",
        "          if pooling=='max_pool':\n",
        "              wdim1=16\n",
        "          else:\n",
        "              wdim1=32\n",
        "      else:\n",
        "          wdim1=32\n",
        "          \n",
        "      stdNeu=tf.placeholder_with_default(logsampler(10**-5,10**-2) ,shape=()) \n",
        "      wd1 = tf.Variable(tf.truncated_normal([wdim1,1], mean=0, stddev=stdNeu), name='w2')\n",
        "      bd1 = tf.Variable(tf.truncated_normal([1], mean=0, stddev=stdNeu), name='b2')\n",
        "\n",
        "\n",
        "      xconv = convolution(data_X,num_input_channels,num_filters,filter_shape,conv_weights,conv_bias,wd1,bd1,W,b,pooling,neuType,training,dropprob)\n",
        "\n",
        "\n",
        "      sig = tf.nn.sigmoid(xconv)\n",
        "      if neuType == 'hidden':\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=data_y,logits=xconv))+ beta1*tf.norm(conv_weights,ord=1)+ beta2*tf.norm(wd1,ord=1)+ beta3*tf.norm(W,ord=1)\n",
        "      else:\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=data_y,logits=xconv))+ beta1*tf.norm(conv_weights,ord=1)+ beta2*tf.norm(wd1,ord=1)\n",
        "\n",
        "      optimizer=tf.train.MomentumOptimizer(learning_rate,momentum_rate,use_nesterov=True).minimize(loss)\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "      #Set up iterator for the validation data\n",
        "      dataset_val = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "      dataset_val = dataset_val.batch(tf.cast(tf.size(y),tf.int64))\n",
        "                  \n",
        "      iterator_val = dataset_val.make_initializable_iterator()\n",
        "      data_XV, data_yV = iterator_val.get_next()      \n",
        "      data_yV = tf.cast(data_yV, tf.float32)\n",
        "    with tf.device('/gpu:0'):\n",
        "      xconvV = convolution(data_XV,num_input_channels,num_filters,filter_shape,conv_weights,conv_bias,wd1,bd1,W,b,pooling,neuType,training,dropprob)\n",
        "\n",
        "      sigV = tf.nn.sigmoid(xconvV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T13:02:14.251812Z",
          "start_time": "2021-06-01T12:50:21.090782Z"
        },
        "id": "yY9abt5qkWsB"
      },
      "source": [
        "import copy\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    dropoutList=[0.5,0.75,1.0] #list of possible dropout values\n",
        "    best_AUC=0\n",
        "\n",
        "    for iter in range(10):\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        beta_1=sess.run(beta1)\n",
        "        beta_2=sess.run(beta2)\n",
        "        beta_3=sess.run(beta3)\n",
        "        lea_r,mom_r,stdc,stdn=sess.run([learning_rate,momentum_rate,stdConv,stdNeu])\n",
        "                \n",
        "        \n",
        "        prob=random.choice(dropoutList)\n",
        "        \n",
        "        crossV=[0,1,2]\n",
        "    \n",
        "        CV_auc_list=[]\n",
        "        Avg_List=[]\n",
        "        for c in crossV:\n",
        "              sess.run(tf.global_variables_initializer())\n",
        "              sess.run(tf.local_variables_initializer())\n",
        "              sess.run([conv_weights,wd1,conv_bias,bd1], feed_dict={stdConv:stdc,stdNeu:stdn})\n",
        "              t=copy.copy(crossV)\n",
        "              t.remove(c)\n",
        "              traind=np.concatenate((data[t[0]], data[t[1]]), axis=0)\n",
        "              labeltrain=np.concatenate((label[t[0]], label[t[1]]), axis=0)\n",
        "\n",
        "              testd=data[c]\n",
        "              labeltest=label[c]\n",
        "\n",
        "\n",
        "\n",
        "              avg_cost=0\n",
        "              auc_list=[]\n",
        "              iterationSteps=0\n",
        "              sess.run(iterator.initializer, feed_dict = {x: traind, y: labeltrain})\n",
        "              try:\n",
        "\n",
        "                  while iterationSteps <=20000:\n",
        "                          iterationSteps+=1\n",
        "                      \n",
        "                          ### Training\n",
        "                          _,lss=sess.run([optimizer,loss], feed_dict= {training: 1, dropprob: prob, beta1:beta_1, \n",
        "                                                                       beta2: beta_2, beta3:beta_3, learning_rate:lea_r, momentum_rate:mom_r,stdConv:stdc,stdNeu:stdn})\n",
        "                         \n",
        "                          \n",
        "                          if iterationSteps % 4000==0:\n",
        "                                  ## Validation\n",
        "\n",
        "                                  sess.run(iterator_val.initializer, feed_dict = {x: testd, y: labeltest})\n",
        "\n",
        "                                  l,yl=sess.run([sigV, data_yV], feed_dict= {training: 0, dropprob: prob, beta1:beta_1, \n",
        "                                                                       beta2: beta_2, beta3:beta_3, learning_rate:lea_r, momentum_rate:mom_r,stdConv:stdc,stdNeu:stdn})\n",
        "                                  auc=metrics.roc_auc_score(yl, l)\n",
        "                                  print('AUC for the number of iterations',iterationSteps,'is:',auc)\n",
        "                                  auc_list.append(auc)\n",
        "\n",
        "\n",
        "              except tf.errors.OutOfRangeError:\n",
        "                  pass\n",
        "              print('===== Fold Done =====') \n",
        "              CV_auc_list.append(auc_list)\n",
        "              \n",
        "        print('The Cross Validation AUC for The Three Folds in 5 Different Iteration Steps:' , CV_auc_list)\n",
        "        for i in range(len(auc_list)):\n",
        "                Avg_List.append(np.mean([CV_auc_list[j][i] for j in range(len(CV_auc_list))]))\n",
        "        print('The Average AUC for each Iteration Step of The Three Folds is:', Avg_List)\n",
        "        \n",
        "    \n",
        "        \n",
        "        maxlist=max(Avg_List)\n",
        "        if maxlist>best_AUC:\n",
        "          best_AUC=maxlist\n",
        "          \n",
        "          ind=Avg_List.index(maxlist)\n",
        "          \n",
        "          lr,mr,sc,sn,b1,b2,b3 = sess.run([learning_rate, momentum_rate,stdConv, stdNeu,beta1,beta2,beta3], feed_dict= {training: 0, dropprob: prob, beta1:beta_1, \n",
        "                                                                       beta2: beta_2, beta3:beta_3, learning_rate:lea_r, momentum_rate:mom_r,stdConv:stdc,stdNeu:stdn})\n",
        "          print( 'Best hyperparameters So far:')\n",
        "          print( 'Best Learning Rate', lr)\n",
        "          print( 'Best Momentum Rate', mr)\n",
        "          print( 'Best Learning Step', (ind+1)*4000)\n",
        "          print( 'Best Sigma Conv', sc)\n",
        "          print( 'Best Sigma NN', sn)\n",
        "          print( 'Best Dropout Prob', prob)\n",
        "          print( 'Best Beta 1', b1)\n",
        "          print( 'Best Beta 2', b2)\n",
        "          print( 'Best Beta 3', b3)\n",
        "          \n",
        "          save_LearningRate=lr\n",
        "          save_Momentum=mr\n",
        "          save_LearningStep=(ind+1)*4000\n",
        "          save_SigmaConv=sc\n",
        "          save_SigmaNeu=sn\n",
        "          save_Dropprob=prob\n",
        "          save_Beta1=b1\n",
        "          save_Beta2=b2\n",
        "          save_Beta3=b3\n",
        "          \n",
        "          \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T13:05:07.738480Z",
          "start_time": "2021-06-01T13:05:07.544195Z"
        },
        "id": "XZK-IaoTkYBK"
      },
      "source": [
        "graph2=tf.Graph()\n",
        "with graph2.as_default():\n",
        "    \n",
        "    num_input_channels=4\n",
        "    num_filters=16\n",
        "    filter_shape=24\n",
        "    pooling='max_pool'\n",
        "    neuType='hidden'\n",
        "    \n",
        "    beta1=save_Beta1\n",
        "    beta2=save_Beta2\n",
        "    beta3=save_Beta3\n",
        "\n",
        "    \n",
        "    \n",
        "    learning_rate= save_LearningRate\n",
        "    momentum_rate= save_Momentum\n",
        "    batch_size=64\n",
        "    \n",
        "    \n",
        "    with tf.device('/gpu:0'):\n",
        "    \n",
        "      x = tf.placeholder(tf.float32, [None, 147, 4],name='X')\n",
        "      y = tf.placeholder(tf.float32,[None,1],name='y')\n",
        "\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "      \n",
        "      #Set up iterator for the data\n",
        "      dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "      dataset = dataset.shuffle(500).repeat().batch(batch_size)\n",
        "      iterator = dataset.make_initializable_iterator()\n",
        "      data_X, data_y = iterator.get_next()\n",
        "      data_y = tf.cast(data_y, tf.float32)\n",
        "\n",
        "      dropprob = tf.placeholder_with_default(0.5, shape=(),name='prob')\n",
        "\n",
        "      # Distinguish training and testing: training=1 for training , =0 for testing\n",
        "      training = tf.placeholder_with_default(0.0, shape=(),name='training')\n",
        "      \n",
        "   \n",
        "    with tf.device('/gpu:0'):\n",
        "      \n",
        "      conv_filt_shape = [filter_shape, num_input_channels, num_filters]\n",
        "\n",
        "      stdConv=save_SigmaConv\n",
        "      # initialise weights and bias for the filter\n",
        "      conv_weights = tf.Variable(tf.truncated_normal(conv_filt_shape, mean=0,stddev=stdConv), name='Conv1_W')\n",
        "      conv_bias = tf.Variable(tf.truncated_normal([num_filters]), name='Conv1_b')\n",
        "\n",
        "\n",
        "\n",
        "      if pooling=='max_pool':\n",
        "          W = tf.Variable(tf.truncated_normal([16,32], mean=0, stddev=0.3), name='W')\n",
        "          b = tf.Variable(tf.truncated_normal([32], mean=0, stddev=0.3), name='b')\n",
        "      else:\n",
        "          W = tf.Variable(tf.truncated_normal([32,32], mean=0, stddev=0.3), name='W')\n",
        "          b = tf.Variable(tf.truncated_normal([32], mean=0, stddev=0.3), name='b')     \n",
        "\n",
        "      if neuType == 'nohidden':\n",
        "          if pooling=='max_pool':\n",
        "              wdim1=16\n",
        "          else:\n",
        "              wdim1=32\n",
        "      else:\n",
        "          wdim1=32\n",
        "      stdNeu=save_SigmaNeu\n",
        "      wd1 = tf.Variable(tf.truncated_normal([wdim1,1], mean=0, stddev=stdNeu), name='w2')\n",
        "      bd1 = tf.Variable(tf.truncated_normal([1], mean=0, stddev=stdNeu), name='b2')\n",
        "\n",
        "\n",
        "      xconv = convolution(data_X,num_input_channels,num_filters,filter_shape,conv_weights,conv_bias,wd1,bd1,W,b,pooling,neuType,training,dropprob)\n",
        "\n",
        "      sig = tf.nn.sigmoid(xconv)\n",
        "      if neuType == 'hidden':\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=data_y,logits=xconv))+ beta1*tf.norm(conv_weights,ord=1)+ beta2*tf.norm(wd1,ord=1)+ beta3*tf.norm(W,ord=1)\n",
        "      else:\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=data_y,logits=xconv))+ beta1*tf.norm(conv_weights,ord=1)+ beta2*tf.norm(wd1,ord=1)\n",
        "\n",
        "      optimizer=tf.train.MomentumOptimizer(learning_rate,momentum_rate,use_nesterov=True).minimize(loss)\n",
        "    with tf.device('/cpu:0'):\n",
        "      \n",
        "      #Set up iterator for the validation data\n",
        "      dataset_val = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "      dataset_val = dataset_val.batch(tf.cast(tf.size(y),tf.int64))\n",
        "                  \n",
        "      iterator_val = dataset_val.make_initializable_iterator()\n",
        "      data_XV, data_yV = iterator_val.get_next()      \n",
        "      data_yV = tf.cast(data_yV, tf.float32)\n",
        "      \n",
        "      \n",
        "      data_XV = tf.placeholder_with_default(data_XV, shape=None, name='input')\n",
        "      data_yV = tf.placeholder_with_default(data_yV, shape=None,name='label')\n",
        "      \n",
        "      \n",
        "    with tf.device('/gpu:0'):\n",
        "      xconvV = convolution(data_XV,num_input_channels,num_filters,filter_shape,conv_weights,conv_bias,wd1,bd1,W,b,pooling,neuType,training,dropprob)\n",
        "\n",
        "      sigV = tf.nn.sigmoid(xconvV, name='Conv_V')\n",
        "  \n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T13:39:41.900695Z",
          "start_time": "2021-06-01T13:38:25.515580Z"
        },
        "id": "huj8k7YtXGXu"
      },
      "source": [
        "import copy\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session(graph=graph2, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    auc_list=[]\n",
        "    best_auc=0\n",
        "    for iter in range(6):\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        \n",
        "\n",
        "        prob=save_Dropprob\n",
        "        iterationSteps=0\n",
        "        sess.run(iterator.initializer, feed_dict = {x: data_all, y: label_all})\n",
        "        try:\n",
        "\n",
        "            while iterationSteps <=save_LearningStep:\n",
        "                  iterationSteps+=1\n",
        "              \n",
        "                  ### Training\n",
        "                  _,lss=sess.run([optimizer,loss], feed_dict= {training: 1, dropprob: prob})\n",
        "                  \n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "          \n",
        "        ## Validation\n",
        "        sess.run(iterator_val.initializer, feed_dict = {x: data_all, y: label_all})\n",
        "        l,yl=sess.run([sigV,data_yV], feed_dict= {training: 0, dropprob: prob})\n",
        "\n",
        "        auc=metrics.roc_auc_score(yl, l)\n",
        "        print('AUC of Model Num',iter,' is : ', auc)\n",
        "        \n",
        "        if auc > best_auc:\n",
        "          best_auc=auc\n",
        "          print('Best AUC So Far is : ', best_auc)\n",
        "          ##save model\n",
        "          save_path = saver.save(sess, \"/home/julian/Documents/bat/DeepBind_with_Tensorflow/models\")\n",
        "          print('Model Saved!')\n",
        "\n",
        "          \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T13:42:35.827373Z",
          "start_time": "2021-06-01T13:42:35.825661Z"
        },
        "id": "P4ES6bgYBbKh"
      },
      "source": [
        "import copy\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T14:07:50.185544Z",
          "start_time": "2021-06-01T14:07:50.183780Z"
        },
        "id": "8vr5DH5ER0k2"
      },
      "source": [
        "filename='/home/julian/Documents/bat/DeepBind_train/data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_B.seq.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T14:10:44.201631Z",
          "start_time": "2021-06-01T14:10:44.198619Z"
        },
        "id": "5cRQzE9-YsAd"
      },
      "source": [
        "class ChipTest(Experiment):\n",
        "    def __init__(self,filename,motiflen=24):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "            \n",
        "    def openFile(self):\n",
        "        train_dataset=[]\n",
        "     \n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            \n",
        "            for row in reader:\n",
        "                    train_dataset.append([seqtopad(row[2],self.motiflen),[row[3]]])\n",
        "                    \n",
        "                   \n",
        "        \n",
        "\n",
        "        return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T14:13:38.405102Z",
          "start_time": "2021-06-01T14:13:37.718882Z"
        },
        "id": "WbQxI6v4R2cb"
      },
      "source": [
        "test= ChipTest(filename)\n",
        "dataAll =test.openFile()\n",
        "data_all=np.asarray([el[0] for el in dataAll],dtype=np.float32)\n",
        "label_all=np.asarray([el[1] for el in dataAll],dtype=np.float32).reshape(len(data_all),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-01T13:51:16.706042Z",
          "start_time": "2021-06-01T13:41:54.333Z"
        },
        "id": "zO0bp4gHxTWw"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "TestGraph=tf.Graph()\n",
        "with tf.Session(graph=TestGraph) as sess:    \n",
        "  \n",
        "  # #First let's load meta graph and restore weights\n",
        "  ckpt = tf.train.get_checkpoint_state('/content/drive/My Drive/Colab Notebooks/Test2', latest_filename='checkpoint')\n",
        "  \n",
        "  if ckpt and ckpt.model_checkpoint_path:  # if there's checkpoint\n",
        "    saver = tf.train.import_meta_graph('/content/drive/My Drive/Colab Notebooks/Test2/model2.meta')\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "\n",
        "    X = TestGraph.get_tensor_by_name(\"input:0\")\n",
        "    y = TestGraph.get_tensor_by_name(\"label:0\")\n",
        "\n",
        "\n",
        "    training = TestGraph.get_tensor_by_name(\"training:0\")\n",
        "    prob = TestGraph.get_tensor_by_name(\"prob:0\")\n",
        "\n",
        "    # #Now, access the op to run. \n",
        "    Conv_V = TestGraph.get_tensor_by_name(\"Conv_V:0\")\n",
        "    feed_dict2={X:data_all,y:label_all,prob:save_Dropprob}\n",
        "    l=sess.run(Conv_V,feed_dict2)\n",
        "    auc=metrics.roc_auc_score(label_all, l)\n",
        "    print(auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-FVFwdZoNwxS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}