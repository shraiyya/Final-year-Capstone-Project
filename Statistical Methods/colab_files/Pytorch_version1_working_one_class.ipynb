{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_version1_working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhikasethi2011/btechproj/blob/main/colab_files/Pytorch_version1_working_one_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-r3oa46w5E"
      },
      "source": [
        "https://github.com/MedChaabane/DeepBind-with-PyTorch/blob/master/Binding_sites_Prediction_PyTorch_colab.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Goiz940CV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "64e460d9-5c21-45ce-b489-0dc81969ac82"
      },
      "source": [
        "'''\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom os import path\\nfrom wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\\nplatform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\\n\\naccelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\\n\\n!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\\nimport torch\\nprint(torch.__version__)\\nprint(torch.cuda.is_available())\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeIU5kFp0H1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d361d8-2172-41ac-e359-3e7ef65a0629"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ3u9Tweh2jw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200515f8-8b54-4ea2-fde3-2d8cb47f5199"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "       process = psutil.Process(os.getpid())\n",
        "       print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "       print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 119.1 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHI_wW1cz8FN"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math \n",
        "import random\n",
        "import gzip\n",
        "from scipy.stats import bernoulli\n",
        "import torch\n",
        "from sklearn import metrics"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mOZziHBz8FT"
      },
      "source": [
        "nummotif=16 #number of motifs to discover\n",
        "bases='ACGT' #DNA bases\n",
        "basesRNA='ACGU'#RNA bases\n",
        "batch_size=64 #fixed batch size -> see notes to problem about it\n",
        "dictReverse={'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode\n",
        "reverse_mode=False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ejzuhv62OgLA",
        "outputId": "4e379191-48e8-4dd7-c9c2-e4d60be09389"
      },
      "source": [
        " import tensorflow as tf \n",
        " tf.test.gpu_device_name() "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJn4P52Fz8FX"
      },
      "source": [
        "def seqtopad(sequence,motlen,kind='DNA'):\n",
        "    rows=len(sequence)+2*motlen-2\n",
        "    S=np.empty([rows,4])\n",
        "    base= bases if kind=='DNA' else basesRNA\n",
        "    for i in range(rows):\n",
        "        for j in range(4):\n",
        "            if i-motlen+1<len(sequence) and sequence[i-motlen+1]=='N' or i<motlen-1 or i>len(sequence)+motlen-2:\n",
        "                S[i,j]=np.float32(0.25)\n",
        "            elif sequence[i-motlen+1]==base[j]:\n",
        "                S[i,j]=np.float32(1)\n",
        "            else:\n",
        "                S[i,j]=np.float32(0)\n",
        "    return np.transpose(S)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBcVVpC8z8Fb"
      },
      "source": [
        "def dinucshuffle(sequence):\n",
        "    b=[sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
        "    random.shuffle(b)\n",
        "    d=''.join([str(x) for x in b])\n",
        "    return d"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGWkcTgD7xrE"
      },
      "source": [
        "def complement(seq):\n",
        "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}\n",
        "    complseq = [complement[base] for base in seq]\n",
        "    return complseq\n",
        "  \n",
        "def reverse_complement(seq):\n",
        "    seq = list(seq)\n",
        "    seq.reverse()\n",
        "    return ''.join(complement(seq))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7LnslLYz8Fj"
      },
      "source": [
        "class Chip():\n",
        "    def __init__(self,filename,motiflen=24,reverse_complemet_mode=reverse_mode):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "            \n",
        "    def openFile(self):\n",
        "        train_dataset=[]\n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            if not self.reverse_complemet_mode:\n",
        "              for row in reader:\n",
        "                      train_dataset.append([seqtopad(row[2],self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(row[2]),self.motiflen),[0]])\n",
        "            else:\n",
        "              for row in reader:\n",
        "                      train_dataset.append([seqtopad(row[2],self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(reverse_complement(row[2]),self.motiflen),[1]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(row[2]),self.motiflen),[0]])\n",
        "                      train_dataset.append([seqtopad(dinucshuffle(reverse_complement(row[2])),self.motiflen),[0]])\n",
        "        #random.shuffle(train_dataset)\n",
        "        train_dataset_pad=train_dataset\n",
        "\n",
        "        size=int(len(train_dataset_pad)/3)\n",
        "        firstvalid=train_dataset_pad[:size]\n",
        "        secondvalid=train_dataset_pad[size:size+size]\n",
        "        thirdvalid=train_dataset_pad[size+size:]\n",
        "        firsttrain=secondvalid+thirdvalid\n",
        "        secondtrain=firstvalid+thirdvalid\n",
        "        thirdtrain=firstvalid+secondvalid\n",
        "        return firsttrain,firstvalid,secondtrain,secondvalid,thirdtrain,thirdvalid,train_dataset_pad"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khK_nGE548jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409d42ff-88b8-4a2f-bd8a-cb84cf0c0585"
      },
      "source": [
        "!git clone \"https://github.com/jisraeli/DeepBind.git\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepBind' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16tHTpImbN6h"
      },
      "source": [
        "! gzip '/content/sampledataset2onlymotif.seq'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwYhmKt6z8Fn"
      },
      "source": [
        "# chipseq=Chip('/content/drive/My Drive/Deepbind/ZBTB7A_HepG2_ZBTB7A_(SC-34508)_HudsonAlpha_AC.seq.gz')\n",
        "chipseq=Chip('/content/sampledataset2onlymotif.seq.gz')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bn0w6urz8Fs"
      },
      "source": [
        "train1,valid1,train2,valid2,train3,valid3,alldataset=chipseq.openFile()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tei03zi9gz76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3b2574-9177-41b1-c2d9-bb51dd61af96"
      },
      "source": [
        "print(train1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX3Kgiaoz8Fw"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class chipseq_dataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,xy=None):\n",
        "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
        "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
        "        self.x_data = torch.from_numpy(self.x_data)\n",
        "        self.y_data = torch.from_numpy(self.y_data)\n",
        "        self.len=len(self.x_data)\n",
        "      \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8SvGe1d4hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40b51d1-333c-46fa-f829-c4efa63da08c"
      },
      "source": [
        "np.array(train1).shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36283, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6k2xjIFeBcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c11477-f7ea-4cf8-e9da-241c6785ed19"
      },
      "source": [
        "np.array(valid1).shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18141, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_F4vtcz8F2"
      },
      "source": [
        "import numpy as np \n",
        "train1_dataset=chipseq_dataset(train1)\n",
        "train2_dataset=chipseq_dataset(train2)\n",
        "train3_dataset=chipseq_dataset(train3)\n",
        "valid1_dataset=chipseq_dataset(valid1)\n",
        "valid2_dataset=chipseq_dataset(valid2)\n",
        "valid3_dataset=chipseq_dataset(valid3)\n",
        "alldataset_dataset=chipseq_dataset(alldataset)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfxIcGZoz8F9"
      },
      "source": [
        "batchSize=64\n",
        "if reverse_mode:\n",
        "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  alldataset_loader=DataLoader(dataset=alldataset_dataset,batch_size=batchSize,shuffle=False)\n",
        "else:\n",
        "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batchSize,shuffle=True)\n",
        "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batchSize,shuffle=True)\n",
        "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batchSize,shuffle=True)\n",
        "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batchSize,shuffle=False)\n",
        "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batchSize,shuffle=False)\n",
        "  alldataset_loader=DataLoader(dataset=alldataset_dataset,batch_size=batchSize,shuffle=False)\n",
        "\n",
        "train_dataloader=[train_loader1,train_loader2,train_loader3]\n",
        "valid_dataloader=[valid1_loader,valid2_loader,valid3_loader]\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHms3Olpz8GA"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "import torch.nn.functional as F\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "def logsampler(a,b):\n",
        "        x=np.random.uniform(low=0,high=1)\n",
        "        y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
        "        return y\n",
        "    \n",
        "def sqrtsampler(a,b):\n",
        "        \n",
        "        x=np.random.uniform(low=0,high=1)\n",
        "        y=(b-a)*math.sqrt(x)+a\n",
        "        return y\n",
        "      \n",
        "# input of shape(batch_size,inp_chan,iW)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, nummotif,motiflen,poolType,neuType,mode,dropprob,learning_rate,momentum_rate,sigmaConv,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode=reverse_mode):\n",
        "      \n",
        "        super(ConvNet, self).__init__()\n",
        "        self.poolType=poolType\n",
        "        self.neuType=neuType\n",
        "        self.mode=mode\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "        self.dropprob=dropprob\n",
        "        self.learning_rate=learning_rate\n",
        "        self.momentum_rate=momentum_rate\n",
        "        self.sigmaConv=sigmaConv\n",
        "        self.sigmaNeu=sigmaNeu\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.beta3=beta3\n",
        "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
        "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
        "        self.wConv.requires_grad=True\n",
        "        \n",
        "        \n",
        "        self.wRect=torch.randn(nummotif).to(device)\n",
        "        torch.nn.init.normal_(self.wRect)\n",
        "        self.wRect=-self.wRect\n",
        "        self.wRect.requires_grad=True\n",
        "        \n",
        "        \n",
        "        if neuType=='nohidden':\n",
        "            \n",
        "            if poolType=='maxavg':\n",
        "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
        "            else:\n",
        "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "\n",
        "        else:\n",
        "            if poolType=='maxavg':\n",
        "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "            else:\n",
        "                \n",
        "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
        "            self.wNeu=torch.randn(32,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            self.wHiddenBias=torch.randn(32).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
        "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
        "            \n",
        "  \n",
        "            self.wHidden.requires_grad=True\n",
        "            self.wHiddenBias.requires_grad=True\n",
        "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
        "\n",
        "        self.wNeu.requires_grad=True\n",
        "        self.wNeuBias.requires_grad=True\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "   \n",
        "    def divide_two_tensors(self,x):\n",
        "        l=torch.unbind(x)\n",
        "\n",
        "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
        "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
        "        x1=torch.stack(list1,0)\n",
        "        x2=torch.stack(list2,0)\n",
        "        return x1,x2\n",
        "    def forward_pass(self,x,mask=None,use_mask=False):\n",
        "        \n",
        "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
        "        rect=conv.clamp(min=0)\n",
        "        maxPool, _ = torch.max(rect, dim=2)\n",
        "        if self.poolType=='maxavg':\n",
        "            avgPool= torch.mean(rect, dim=2)                          \n",
        "            pool=torch.cat((maxPool, avgPool), 1)\n",
        "        else:\n",
        "            pool=maxPool\n",
        "        if(self.neuType=='nohidden'):\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                  mask=bernoulli.rvs(self.dropprob, size=len(pool[0]))\n",
        "                  mask=torch.from_numpy(mask).float().to(device)\n",
        "                pooldrop=pool*mask\n",
        "                out=pooldrop @ self.wNeu\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(pool @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)       \n",
        "        else:\n",
        "            hid=pool @ self.wHidden\n",
        "            hid.add_(self.wHiddenBias)\n",
        "            hid=hid.clamp(min=0)\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                  mask=bernoulli.rvs(self.dropprob, size=len(hid[0]))\n",
        "                  mask=torch.from_numpy(mask).float().to(device)\n",
        "                hiddrop=hid*mask\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias) \n",
        "        return out,mask\n",
        "       \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not  self.reverse_complemet_mode:\n",
        "            out,_=self.forward_pass(x)\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            x1,x2=self.divide_two_tensors(x)\n",
        "            out1,mask=self.forward_pass(x1)\n",
        "            out2,_=self.forward_pass(x2,mask,True)\n",
        "            out=torch.max(out1, out2)\n",
        "     \n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ACNZE2z8GE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7896bb-5cf6-466c-d68e-81179125ae31"
      },
      "source": [
        "best_AUC=0\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# device='cpu'\n",
        "\n",
        "learning_steps_list=[4000,8000,12000,16000,20000]\n",
        "\n",
        "for number in range(5):\n",
        "    \n",
        "    pool_List=['max','maxavg']        \n",
        "    random_pool=random.choice(pool_List)\n",
        "    \n",
        "    neuType_list=['hidden','nohidden']\n",
        "    random_neuType=random.choice(neuType_list)\n",
        "    dropoutList=[0.5,0.75,1.0] \n",
        "        \n",
        "    dropprob=random.choice(dropoutList)\n",
        "    \n",
        "    learning_rate=logsampler(0.0005,0.05)\n",
        " \n",
        "    momentum_rate=sqrtsampler(0.95,0.99)  \n",
        "\n",
        "    sigmaConv=logsampler(10**-7,10**-3)   \n",
        "    \n",
        "\n",
        "    sigmaNeu=logsampler(10**-5,10**-2) \n",
        "    beta1=logsampler(10**-15,10**-3)\n",
        "    beta2=logsampler(10**-10,10**-3)\n",
        "    beta3=logsampler(10**-10,10**-3)\n",
        "             \n",
        "    model_auc=[[],[],[]]\n",
        "    for kk in range(3):\n",
        "        model = ConvNet(16,24,random_pool,random_neuType,'training',dropprob,learning_rate,momentum_rate,sigmaConv,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode=reverse_mode).to(device)\n",
        "        if random_neuType=='nohidden':\n",
        "            optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "        else:\n",
        "            optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "        train_loader=train_dataloader[kk]\n",
        "        valid_loader=valid_dataloader[kk]\n",
        "        learning_steps=0\n",
        "        while learning_steps<=20000:\n",
        "            model.mode='training'\n",
        "            auc=[]\n",
        "            for i, (data, target) in enumerate(train_loader):\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                if model.reverse_complemet_mode:\n",
        "                  target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "                  for i in range(target_2.shape[0]):\n",
        "                    target_2[i]=target[2*i]\n",
        "                  target=target_2.to(device)\n",
        "                \n",
        "     \n",
        "                # Forward pass\n",
        "                output = model(data)\n",
        "                if model.neuType=='nohidden':\n",
        "                    loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "                else:\n",
        "                    loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta2*model.wHidden.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                learning_steps+=1\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "                if learning_steps% 4000==0:\n",
        "       \n",
        "                    with torch.no_grad():\n",
        "                        model.mode='test'\n",
        "                        auc=[]\n",
        "                        for i, (data, target) in enumerate(valid_loader):\n",
        "                            data = data.to(device)\n",
        "                            target = target.to(device)\n",
        "                            if model.reverse_complemet_mode:\n",
        "                              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "                              for i in range(target_2.shape[0]):\n",
        "                                target_2[i]=target[2*i]\n",
        "                              target=target_2.to(device)\n",
        "                            # Forward pass\n",
        "                            output = model(data)\n",
        "                            pred_sig=torch.sigmoid(output)\n",
        "                            pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "                            labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "                            \n",
        "                            auc.append(metrics.roc_auc_score(labels, pred))\n",
        "#                         print(np.mean(auc))\n",
        "                        model_auc[kk].append(np.mean(auc))\n",
        "                        print('AUC performance when training fold number ',kk+1, 'using ',learning_steps_list[len(model_auc[kk])-1],'learning steps = ',np.mean(auc))\n",
        "    \n",
        "    print('                   ##########################################               ')\n",
        "    for n in range(5):\n",
        "        AUC=(model_auc[0][n]+model_auc[1][n]+model_auc[2][n])/3\n",
        "        #print(AUC)\n",
        "        if AUC>best_AUC:\n",
        "            best_AUC=AUC\n",
        "            best_learning_steps=learning_steps_list[n]\n",
        "            best_LearningRate=model.learning_rate\n",
        "            best_LearningMomentum=model.momentum_rate\n",
        "            best_neuType=model.neuType\n",
        "            best_poolType=model.poolType\n",
        "            best_sigmaConv=model.sigmaConv\n",
        "            best_dropprob=model.dropprob\n",
        "            best_sigmaNeu=model.sigmaNeu\n",
        "            best_beta1=model.beta1\n",
        "            best_beta2=model.beta2\n",
        "            best_beta3=model.beta3\n",
        "            \n",
        "print('best_poolType=',best_poolType)\n",
        "print('best_neuType=',best_neuType)\n",
        "print('best_AUC=',best_AUC)            \n",
        "print('best_learning_steps=',best_learning_steps)      \n",
        "print('best_LearningRate=',best_LearningRate)\n",
        "print('best_LearningMomentum=',best_LearningMomentum)\n",
        "print('best_sigmaConv=',best_sigmaConv)\n",
        "print('best_dropprob=',best_dropprob)\n",
        "print('best_sigmaNeu=',best_sigmaNeu)\n",
        "print('best_beta1=',best_beta1)\n",
        "print('best_beta2=',best_beta2)\n",
        "print('best_beta3=',best_beta3)\n",
        "\n",
        "best_hyperparameters = {'best_poolType': best_poolType,'best_neuType':best_neuType,'best_learning_steps':best_learning_steps,'best_LearningRate':best_LearningRate,\n",
        "                        'best_LearningMomentum':best_LearningMomentum,'best_sigmaConv':best_sigmaConv,'best_dropprob':best_dropprob,\n",
        "                        'best_sigmaNeu':best_sigmaNeu,'best_beta1':best_beta1, 'best_beta2':best_beta2,'best_beta3':best_beta3}\n",
        "torch.save(best_hyperparameters, '/content/drive/My Drive/best_hyperpamarameters.pth')\n",
        "            \n",
        " "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.9643279271986083\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.9655164057260228\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.9653483072916667\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.9660880648474179\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.9626187299578722\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.9667487345950704\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.9676135425836268\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.967164805237676\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.9671991912411971\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.966643857284331\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.9673921654929577\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.970460237553795\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.9707146939798512\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.9713549384414124\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.971480485560935\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.9650663748113683\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.966045065968729\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.9661602590805247\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.9666296280285881\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.9672636404363683\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.9671304192341549\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.9687362455985915\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.968688105193662\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.9679763149207746\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.9679522447183099\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.9672512592918623\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.967738272080399\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.9687869076437794\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.9694440318124021\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.9698982785602505\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.9609753246038732\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.9600292510269954\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.9622372909330986\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.9607161196344736\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.9615843825976694\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.9647784165933099\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.9643726617517606\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.9641182053257042\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.9638671875\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.9638843805017606\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.9593444346635367\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.9632393907717135\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.9620332825948749\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.9624990371919014\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.9630650766578639\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.9658082446501928\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.9674880336707746\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.9657050866396294\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.9637622774406858\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.9637721675102701\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.9694652288732394\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.9692520356514085\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.9697781415052817\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.9696767027948944\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.9697936152068662\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.9667342007775821\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.9667812484717331\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.9689377781445618\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.9683881293402778\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.9689917794527583\n",
            "                   ##########################################               \n",
            "AUC performance when training fold number  1 using  4000 learning steps =  0.9617317839327632\n",
            "AUC performance when training fold number  1 using  8000 learning steps =  0.9622600511925721\n",
            "AUC performance when training fold number  1 using  12000 learning steps =  0.9618371033492623\n",
            "AUC performance when training fold number  1 using  16000 learning steps =  0.961699526586603\n",
            "AUC performance when training fold number  1 using  20000 learning steps =  0.9612048266159458\n",
            "AUC performance when training fold number  2 using  4000 learning steps =  0.9629593970070423\n",
            "AUC performance when training fold number  2 using  8000 learning steps =  0.9610165878080986\n",
            "AUC performance when training fold number  2 using  12000 learning steps =  0.9613776408450704\n",
            "AUC performance when training fold number  2 using  16000 learning steps =  0.9627290107834507\n",
            "AUC performance when training fold number  2 using  20000 learning steps =  0.9624607999559859\n",
            "AUC performance when training fold number  3 using  4000 learning steps =  0.9597206099007238\n",
            "AUC performance when training fold number  3 using  8000 learning steps =  0.9623107547192878\n",
            "AUC performance when training fold number  3 using  12000 learning steps =  0.9614095663390063\n",
            "AUC performance when training fold number  3 using  16000 learning steps =  0.9622564401163928\n",
            "AUC performance when training fold number  3 using  20000 learning steps =  0.9618065489289906\n",
            "                   ##########################################               \n",
            "best_poolType= max\n",
            "best_neuType= hidden\n",
            "best_AUC= 0.9683713879049761\n",
            "best_learning_steps= 20000\n",
            "best_LearningRate= 0.0005294874699561866\n",
            "best_LearningMomentum= 0.9856391358190306\n",
            "best_sigmaConv= 0.00012939429614477918\n",
            "best_dropprob= 0.75\n",
            "best_sigmaNeu= 0.005293889739698084\n",
            "best_beta1= 1.4756519670484881e-07\n",
            "best_beta2= 8.792182048451807e-09\n",
            "best_beta3= 1.3525112369861437e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwZUA6cmRtlN"
      },
      "source": [
        "# input of shape(batch_size,inp_chan,iW)\n",
        "class ConvNet_test(nn.Module):\n",
        "    def __init__(self,nummotif,motiflen,poolType,neuType,mode,learning_steps,learning_rate,learning_Momentum,sigmaConv,dropprob,sigmaNeu,beta1,beta2,beta3,reverse_complemet_mode):\n",
        "        super(ConvNet_test, self).__init__()\n",
        "        self.poolType=poolType\n",
        "        self.neuType=neuType\n",
        "        self.mode=mode\n",
        "        self.learning_rate=learning_rate\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "        self.momentum_rate=learning_Momentum\n",
        "        self.sigmaConv=sigmaConv\n",
        "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
        "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
        "        self.wConv.requires_grad=True\n",
        "        self.wRect=torch.randn(nummotif).to(device)\n",
        "        torch.nn.init.normal_(self.wRect)\n",
        "        self.wRect=-self.wRect\n",
        "        self.wRect.requires_grad=True\n",
        "        self.dropprob=dropprob\n",
        "        self.sigmaNeu=sigmaNeu\n",
        "        self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "        self.wHiddenBias=torch.randn(32).to(device)\n",
        "        if neuType=='nohidden':\n",
        "            \n",
        "            if poolType=='maxavg':\n",
        "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
        "            else:\n",
        "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "\n",
        "        else:\n",
        "            if poolType=='maxavg':\n",
        "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
        "            else:\n",
        "                \n",
        "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
        "            self.wNeu=torch.randn(32,1).to(device)\n",
        "            self.wNeuBias=torch.randn(1).to(device)\n",
        "            self.wHiddenBias=torch.randn(32).to(device)\n",
        "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
        "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
        "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
        "            \n",
        "  \n",
        "            self.wHidden.requires_grad=True\n",
        "            self.wHiddenBias.requires_grad=True\n",
        "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
        "\n",
        "        self.wNeu.requires_grad=True\n",
        "        self.wNeuBias.requires_grad=True\n",
        "        \n",
        "\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.beta3=beta3\n",
        "        \n",
        "\n",
        "    \n",
        "    def divide_two_tensors(self,x):\n",
        "        l=torch.unbind(x)\n",
        "\n",
        "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
        "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
        "        x1=torch.stack(list1,0)\n",
        "        x2=torch.stack(list2,0)\n",
        "        return x1,x2\n",
        "    def forward_pass(self,x,mask=None,use_mask=False):\n",
        "        \n",
        "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
        "        rect=conv.clamp(min=0)\n",
        "        maxPool, _ = torch.max(rect, dim=2)\n",
        "        if self.poolType=='maxavg':\n",
        "            avgPool= torch.mean(rect, dim=2)                          \n",
        "            pool=torch.cat((maxPool, avgPool), 1)\n",
        "        else:\n",
        "            pool=maxPool\n",
        "        if(self.neuType=='nohidden'):\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                    mask=bernoulli.rvs(self.dropprob, size=len(pool[0]))\n",
        "                    mask=torch.from_numpy(mask).float().to(device)\n",
        "                pooldrop=pool*mask\n",
        "                out=pooldrop @ self.wNeu\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(pool @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)       \n",
        "        else:\n",
        "            hid=pool @ self.wHidden\n",
        "            hid.add_(self.wHiddenBias)\n",
        "            hid=hid.clamp(min=0)\n",
        "            if self.mode=='training': \n",
        "                if  not use_mask:\n",
        "                    mask=bernoulli.rvs(self.dropprob, size=len(hid[0]))\n",
        "                    mask=torch.from_numpy(mask).float().to(device)\n",
        "                hiddrop=hid*mask\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias)\n",
        "            else:\n",
        "                out=self.dropprob*(hid @ self.wNeu)\n",
        "                out.add_(self.wNeuBias) \n",
        "        return out,mask\n",
        "       \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not  self.reverse_complemet_mode:\n",
        "            out,_=self.forward_pass(x)\n",
        "        else:\n",
        "            \n",
        "            x1,x2=self.divide_two_tensors(x)\n",
        "            out1,mask=self.forward_pass(x1)\n",
        "            out2,_=self.forward_pass(x2,mask,True)\n",
        "            out=torch.max(out1, out2)\n",
        "        \n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkrEJTXuz8GW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174ac881-1646-4d67-bfe6-f8a415ad40da"
      },
      "source": [
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "learning_steps_list=[4000,8000,12000,16000,20000]\n",
        "best_AUC=0\n",
        "\n",
        "best_hyperparameters = torch.load('/content/drive/My Drive/best_hyperpamarameters.pth')\n",
        "\n",
        "best_poolType=best_hyperparameters['best_poolType']\n",
        "best_neuType=best_hyperparameters['best_neuType']\n",
        "best_learning_steps=best_hyperparameters['best_learning_steps']\n",
        "best_LearningRate=best_hyperparameters['best_LearningRate']\n",
        "best_dropprob=best_hyperparameters['best_dropprob']\n",
        "best_LearningMomentum=best_hyperparameters['best_LearningMomentum']\n",
        "best_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
        "best_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
        "best_beta1=best_hyperparameters['best_beta1']\n",
        "best_beta2=best_hyperparameters['best_beta2']\n",
        "best_beta3=best_hyperparameters['best_beta3']\n",
        "\n",
        "\n",
        "for number_models in range(6):\n",
        "\n",
        "  model = ConvNet_test(16,24,best_poolType,best_neuType,'training',best_learning_steps,best_LearningRate,best_LearningMomentum,best_sigmaConv,best_dropprob,best_sigmaNeu,best_beta1,best_beta2,best_beta3,reverse_complemet_mode=False).to(device)\n",
        "\n",
        "  if model.neuType=='nohidden':\n",
        "      optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "  else:\n",
        "      optimizer = torch.optim.SGD([model.wConv,model.wRect,model.wNeu,model.wNeuBias,model.wHidden,model.wHiddenBias], lr=model.learning_rate,momentum=model.momentum_rate,nesterov=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_loader=alldataset_loader\n",
        "  valid_loader=alldataset_loader\n",
        "  learning_steps=0\n",
        "  while learning_steps<=best_learning_steps:\n",
        "  \n",
        "      for i, (data, target) in enumerate(train_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "            # Forward pass\n",
        "          output = model(data)\n",
        "          \n",
        "          if model.neuType=='nohidden':\n",
        "              loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta3*model.wNeu.norm()\n",
        "\n",
        "          else:\n",
        "              loss = F.binary_cross_entropy(torch.sigmoid(output),target)+model.beta1*model.wConv.norm()+model.beta2*model.wHidden.norm()+model.beta3*model.wNeu.norm()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          learning_steps+=1\n",
        "          \n",
        "  with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "      for i, (data, target) in enumerate(valid_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "\n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #             \n",
        "      AUC_training=np.mean(auc)\n",
        "      print('AUC for model ',number_models,' = ',AUC_training)\n",
        "      if AUC_training>best_AUC:\n",
        "          state = {'conv': model.wConv,'rect':model.wRect,'wHidden':model.wHidden,'wHiddenBias':model.wHiddenBias,'wNeu':model.wNeu,'wNeuBias':model.wNeuBias }\n",
        "          torch.save(state, '/content/drive/My Drive/MyModel_2.pth')           \n",
        " "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "AUC for model  0  =  0.9811145639525395\n",
            "AUC for model  1  =  0.9782250405976628\n",
            "AUC for model  2  =  0.9776063852575403\n",
            "AUC for model  3  =  0.979368188671008\n",
            "AUC for model  4  =  0.9806454722385429\n",
            "AUC for model  5  =  0.981064199389607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JetF0ajS4faT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0f2f96-b168-4817-bac9-226fd2025d34"
      },
      "source": [
        "checkpoint = torch.load('/content/drive/My Drive/MyModel_2.pth')\n",
        "model = ConvNet_test(16,24,best_poolType,best_neuType,'test',best_learning_steps,best_LearningRate,best_LearningMomentum,best_sigmaConv,best_dropprob,best_sigmaNeu,best_beta1,best_beta2,best_beta3,reverse_complemet_mode=reverse_mode).to(device)\n",
        "model.wConv=checkpoint['conv']\n",
        "model.wRect=checkpoint['rect']\n",
        "model.wHidden=checkpoint['wHidden']\n",
        "model.wHiddenBias=checkpoint['wHiddenBias']\n",
        "model.wNeu=checkpoint['wNeu']\n",
        "model.wNeuBias=checkpoint['wNeuBias']\n",
        "\n",
        "with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "     \n",
        "      for i, (data, target) in enumerate(valid_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "\n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #                         \n",
        "      AUC_training=np.mean(auc)\n",
        "      print(AUC_training)\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.981064199389607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AObJY0ZjjNLW"
      },
      "source": [
        "class Chip_test():\n",
        "    def __init__(self,filename,motiflen=24,reverse_complemet_mode=reverse_mode):\n",
        "        self.file = filename\n",
        "        self.motiflen = motiflen\n",
        "        self.reverse_complemet_mode=reverse_complemet_mode\n",
        "            \n",
        "    def openFile(self):\n",
        "        test_dataset=[]\n",
        "        with gzip.open(self.file, 'rt') as data:\n",
        "            next(data)\n",
        "            reader = csv.reader(data,delimiter='\\t')\n",
        "            if not self.reverse_complemet_mode:\n",
        "              for row in reader:\n",
        "                      test_dataset.append([seqtopad(row[2],self.motiflen),[int(row[3])]])\n",
        "            else:\n",
        "              for row in reader:\n",
        "                      test_dataset.append([seqtopad(row[2],self.motiflen),[int(row[3])]])\n",
        "                      test_dataset.append([seqtopad(reverse_complement(row[2]),self.motiflen),[int(row[3])]])\n",
        "            \n",
        "        return test_dataset\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZObdD8Yaj8w1"
      },
      "source": [
        "chipseq_test=Chip_test('/content/testing1000.seq.gz')\n",
        "test_data=chipseq_test.openFile()\n",
        "test_dataset=chipseq_dataset(test_data)\n",
        "batchSize=test_dataset.__len__()\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batchSize,shuffle=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4oqk1HnfQQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060cc3b0-33f8-4fa2-8cf4-45b42e48c054"
      },
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "      model.mode='test'\n",
        "      auc=[]\n",
        "     \n",
        "      for i, (data, target) in enumerate(test_loader):\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          if model.reverse_complemet_mode:\n",
        "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
        "              for i in range(target_2.shape[0]):\n",
        "                target_2[i]=target[2*i]\n",
        "              target=target_2.to(device)\n",
        "          # Forward pass\n",
        "          output = model(data)\n",
        "          pred_sig=torch.sigmoid(output)\n",
        "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
        "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
        "          \n",
        "          auc.append(metrics.roc_auc_score(labels, pred))\n",
        "  #                         \n",
        "      AUC_training=np.mean(auc)\n",
        "      print('AUC on test data = ',AUC_training)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC on test data =  0.518328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qn3xezvB9eD",
        "outputId": "e2c8894b-0ecb-459d-ed14-b62fe110a02b"
      },
      "source": [
        "model.parameters()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7efde4c95c50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdWr8WjzCBto",
        "outputId": "a46ae603-1dab-45f3-d19c-1b8c5d855975"
      },
      "source": [
        "model.train()\n",
        "model.eval()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet_test()"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ikZ-AICsCV",
        "outputId": "944b92cf-73b4-450b-85fc-7ddaed0aaf81"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet_test()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku91rPD9FUV8"
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/MyModel_2.pth')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VblHS7zmGJtH",
        "outputId": "affadbbd-29b2-4bb1-c1d6-ef05c583a99a"
      },
      "source": [
        "model"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conv': tensor([[[-5.2736e-02, -4.1646e-02, -1.1430e-02,  ..., -7.8168e-02,\n",
              "           -2.9828e-02, -6.6322e-02],\n",
              "          [-5.2755e-02, -9.4662e-02, -1.4921e-01,  ..., -9.4993e-02,\n",
              "           -3.7980e-02, -9.1055e-02],\n",
              "          [-1.3115e-01, -6.4418e-02, -1.5870e-02,  ..., -8.8538e-02,\n",
              "           -1.2923e-01, -8.8229e-02],\n",
              "          [-8.6071e-02, -1.2233e-01, -1.4682e-01,  ..., -6.1774e-02,\n",
              "           -1.2633e-01, -7.7938e-02]],\n",
              " \n",
              "         [[-9.2391e-02, -5.7090e-02, -7.1948e-02,  ..., -5.8197e-02,\n",
              "           -1.2538e-01,  9.2494e-03],\n",
              "          [-1.8511e-02, -1.0739e-01, -3.3456e-02,  ..., -2.1744e-02,\n",
              "            2.7455e-02, -6.3550e-02],\n",
              "          [-4.8541e-02,  3.4374e-02, -4.6435e-02,  ...,  1.9807e-02,\n",
              "           -1.0083e-01, -6.6188e-02],\n",
              "          [-1.7996e-02, -4.7258e-02, -2.5721e-02,  ..., -1.1726e-01,\n",
              "            2.1546e-02, -5.6696e-02]],\n",
              " \n",
              "         [[-2.8021e-05, -2.2005e-04, -1.1434e-04,  ..., -1.1205e-04,\n",
              "            5.4759e-05, -8.1262e-05],\n",
              "          [ 1.5621e-04,  2.2435e-06,  7.1305e-05,  ...,  1.7192e-04,\n",
              "            1.4524e-04, -1.1788e-04],\n",
              "          [ 1.8290e-04,  3.6014e-05, -5.0747e-05,  ...,  2.8578e-04,\n",
              "           -1.0473e-04,  1.9215e-05],\n",
              "          [-5.4191e-05, -1.0920e-04,  1.0258e-04,  ...,  3.5262e-04,\n",
              "           -1.5848e-04, -4.6880e-05]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 9.3690e-05,  2.1563e-04,  1.4149e-04,  ..., -1.1170e-04,\n",
              "            1.0604e-04,  2.4537e-04],\n",
              "          [ 6.2573e-05,  1.3298e-04,  7.9135e-05,  ...,  1.4682e-04,\n",
              "           -1.2919e-05,  1.0058e-04],\n",
              "          [ 4.0671e-05, -8.9026e-05, -1.1872e-04,  ..., -1.8678e-04,\n",
              "           -1.7424e-04,  6.4031e-05],\n",
              "          [ 4.3003e-05,  1.6679e-05,  3.4979e-05,  ...,  7.0737e-05,\n",
              "           -2.0702e-04, -1.5268e-04]],\n",
              " \n",
              "         [[-1.1161e-02, -3.4083e-02, -1.0430e-02,  ...,  1.9383e-02,\n",
              "           -5.4167e-02, -3.8844e-02],\n",
              "          [-8.4021e-02, -1.3249e-02, -6.9576e-02,  ..., -8.1157e-02,\n",
              "           -2.0232e-02, -5.8216e-02],\n",
              "          [-5.2863e-02, -3.6575e-02, -2.2571e-02,  ...,  3.6039e-03,\n",
              "           -5.0727e-02, -3.9541e-02],\n",
              "          [ 4.4475e-03, -5.9293e-02, -4.0981e-02,  ..., -8.5230e-02,\n",
              "           -1.8102e-02, -7.2106e-03]],\n",
              " \n",
              "         [[-7.0186e-02,  4.9175e-02, -2.0395e-02,  ...,  7.2189e-05,\n",
              "            9.0606e-03, -1.3860e-02],\n",
              "          [-2.3885e-02, -3.9659e-02,  2.8297e-03,  ..., -3.7290e-02,\n",
              "           -5.8287e-02, -8.2832e-04],\n",
              "          [ 1.0674e-02, -3.5342e-02,  1.0483e-02,  ..., -5.9364e-02,\n",
              "           -3.7004e-02, -1.1789e-03],\n",
              "          [ 2.4322e-02, -3.2774e-02, -5.2005e-02,  ...,  3.7436e-02,\n",
              "            2.7608e-02, -4.2585e-02]]], device='cuda:0', requires_grad=True),\n",
              " 'rect': tensor([ 0.0949,  0.1940, -0.6032,  0.5695,  0.8356, -0.7892,  0.5364,  0.2265,\n",
              "         -1.5366,  1.2111,  0.2846,  1.5748,  2.0373, -0.1130,  0.2855,  1.0612],\n",
              "        device='cuda:0', requires_grad=True),\n",
              " 'wHidden': tensor([[ 2.8627e-01,  1.6226e-01,  5.2066e-01,  4.4452e-01,  1.9698e-01,\n",
              "          -1.8474e-01,  2.9395e-01,  6.7153e-01, -3.8583e-01, -4.8839e-01,\n",
              "          -1.2821e-01, -2.0018e-02, -1.1510e-02,  5.0778e-01, -6.2660e-01,\n",
              "           8.3928e-01, -4.4525e-01, -4.1107e-01, -9.1533e-01,  4.5329e-02,\n",
              "           6.5454e-01,  8.9618e-01,  5.9147e-01, -1.3509e-01, -7.6451e-01,\n",
              "          -5.2011e-01, -1.9499e-01, -3.6268e-02, -4.6527e-02, -1.1746e+00,\n",
              "          -5.5283e-01, -3.3383e-01],\n",
              "         [-2.6713e-01, -7.5145e-02,  7.0693e-02,  2.2766e-01,  2.3456e-01,\n",
              "           6.1785e-01,  1.4327e-01,  8.5036e-01,  2.1507e-01,  6.0585e-03,\n",
              "           2.2313e-02,  4.3122e-01,  6.2601e-02, -2.6152e-01,  1.8403e-01,\n",
              "           2.4531e-01,  5.5308e-03, -1.2291e-01,  1.4211e-01,  1.2796e-01,\n",
              "           5.5591e-01,  6.1919e-01,  1.1617e-01, -3.5587e-01, -5.6089e-01,\n",
              "           4.4593e-02, -2.2760e-03, -8.5328e-03,  2.0600e-02, -4.9832e-01,\n",
              "          -4.3780e-01, -9.1108e-02],\n",
              "         [-4.6775e-01,  5.0086e-01,  3.0262e-02,  2.0990e-01, -1.5232e-01,\n",
              "          -5.0274e-01,  3.2012e-01, -6.7032e-01, -4.5689e-02,  4.2482e-01,\n",
              "           1.9353e-01,  9.3071e-02,  1.5839e-01,  3.9923e-01,  2.6123e-01,\n",
              "           4.0315e-01, -4.2196e-01, -2.3347e-01,  1.4424e-01, -6.7209e-01,\n",
              "          -4.5113e-01,  7.9665e-02,  2.8250e-01, -3.9488e-01, -7.8073e-02,\n",
              "           1.4929e-01,  5.1485e-02, -1.8496e-01,  6.5621e-01, -1.2911e-01,\n",
              "           8.6792e-03, -5.7016e-02],\n",
              "         [ 1.9141e-01,  3.1052e-02, -2.2203e-01, -4.8264e-02,  1.7251e-02,\n",
              "           5.9151e-03,  2.2559e-01, -1.6798e-01,  6.3110e-01,  2.2767e-01,\n",
              "           3.7125e-01, -4.1432e-01,  2.4549e-01, -2.8296e-01, -6.4188e-01,\n",
              "          -3.2258e-02,  2.0619e-01,  5.3614e-02, -2.8119e-01, -1.1102e-01,\n",
              "          -4.4456e-01, -6.4333e-01, -1.4127e-01, -2.0085e-01,  5.9625e-01,\n",
              "          -1.6638e-01,  3.4734e-01,  3.0025e-01, -1.1425e-01,  6.7101e-04,\n",
              "          -9.3909e-02,  2.6605e-01],\n",
              "         [ 4.4342e-02, -1.6387e-01, -3.1257e-02,  1.3176e-01, -2.7100e-02,\n",
              "           2.3265e-01,  1.6913e-02,  1.7145e-01,  2.0272e-01,  6.7798e-01,\n",
              "          -3.0732e-02,  4.9701e-03, -5.0786e-01,  6.0275e-01, -2.3075e-01,\n",
              "          -2.7843e-01, -2.2152e-01,  4.8078e-01, -2.3587e-01,  1.3790e-02,\n",
              "          -8.8007e-01, -4.8259e-01, -2.0004e-01,  9.5637e-02,  7.5645e-01,\n",
              "          -3.2374e-01,  2.3435e-01,  6.5057e-01,  4.6591e-01, -3.8514e-01,\n",
              "          -5.5527e-01, -8.7161e-02],\n",
              "         [-1.0945e-01,  4.5868e-01,  2.6029e-02, -2.8967e-01, -1.4681e-01,\n",
              "          -4.8190e-01,  4.0911e-01,  2.0787e-01, -2.2013e-01,  3.9273e-01,\n",
              "           1.4483e-01, -1.2563e-01, -2.5461e-01, -4.7759e-02,  3.6095e-02,\n",
              "          -9.4933e-02, -1.2216e-01, -6.8020e-01,  1.5357e-01,  1.6749e-01,\n",
              "           1.1294e-01, -4.5705e-01, -1.9553e-01,  1.0465e-01, -3.6980e-01,\n",
              "          -2.8903e-01, -3.6919e-01, -2.3388e-01,  3.5977e-02,  7.6887e-02,\n",
              "          -2.4519e-01,  1.8212e-01],\n",
              "         [ 2.4269e-01,  1.6236e-01, -2.3041e-01,  9.8533e-02,  1.8924e-01,\n",
              "           5.5979e-01, -7.2852e-01, -7.7286e-02, -5.7581e-02,  2.7200e-02,\n",
              "          -1.1202e-01,  1.5851e-02,  1.7816e-01, -2.2401e-01,  4.1324e-01,\n",
              "          -5.7668e-02, -2.8160e-01,  9.0842e-02,  3.2598e-01, -4.2115e-01,\n",
              "          -4.3731e-01, -4.2989e-01, -2.9522e-01, -2.0026e-01,  7.5487e-01,\n",
              "           4.4735e-01,  2.3953e-01,  2.3431e-01,  5.7988e-01,  1.2887e-01,\n",
              "          -2.5130e-01,  6.0390e-02],\n",
              "         [-1.1533e-01,  2.5201e-01, -4.5759e-01,  5.6763e-01, -7.0142e-01,\n",
              "          -7.8028e-02,  8.4479e-03,  4.5268e-01, -2.7310e-01,  7.3983e-02,\n",
              "           6.4383e-03,  4.5453e-01,  3.3283e-01,  5.7930e-02,  3.2224e-01,\n",
              "           4.7359e-01,  6.7055e-02, -1.8377e-01,  3.1529e-03, -4.9202e-01,\n",
              "           8.0609e-01,  5.3090e-01, -1.7035e-01, -1.9624e-01, -9.9629e-02,\n",
              "          -7.4125e-02,  9.5393e-02, -7.1782e-02,  1.4280e-03, -5.0366e-01,\n",
              "           6.6886e-01,  3.6156e-01],\n",
              "         [-2.1995e-01, -2.4734e-01, -1.1025e-01,  5.0404e-01, -2.3082e-01,\n",
              "           1.2116e-02,  1.5609e-01, -4.6308e-02,  2.0686e-03,  1.8957e-01,\n",
              "           4.3604e-01,  1.5317e-01, -5.0039e-01,  2.9743e-01,  1.2498e-01,\n",
              "           2.1140e-01,  5.9496e-02,  9.2333e-02, -2.0612e-01, -1.5249e-01,\n",
              "          -3.4473e-01,  2.6405e-01,  5.3412e-01,  1.6502e-01, -7.3463e-03,\n",
              "          -3.9341e-01,  7.9736e-02, -2.2096e-01, -1.5647e-01, -1.5451e-01,\n",
              "          -5.5884e-01,  3.1445e-01],\n",
              "         [-3.0719e-02,  1.7037e-01, -5.6472e-01,  2.1226e-01, -1.1270e-02,\n",
              "           1.1771e-01, -4.0209e-01,  4.7174e-01, -1.2022e-01, -1.6362e-01,\n",
              "           2.7350e-01, -4.1639e-01,  1.4241e-01,  4.6196e-01, -1.3520e-01,\n",
              "          -1.8727e-01,  2.2790e-01, -3.7579e-01, -4.1377e-01, -2.2770e-01,\n",
              "          -3.1034e-02, -1.2686e-01,  1.9479e-01, -4.7712e-01, -1.8436e-01,\n",
              "           3.1596e-02, -1.0428e-01, -2.5289e-02, -6.4384e-02,  3.5346e-01,\n",
              "           1.6391e-01, -3.9118e-01],\n",
              "         [ 7.0252e-02, -7.8543e-02,  3.1297e-01, -1.3850e-01, -3.1642e-01,\n",
              "          -9.0878e-02,  1.7333e-01,  2.8011e-03,  1.4613e-01, -3.0962e-01,\n",
              "           4.3906e-01,  8.2798e-04,  2.2748e-01,  2.4937e-02, -7.6670e-01,\n",
              "          -3.2076e-01,  6.5958e-02,  7.2697e-02,  1.1502e-01,  5.6838e-01,\n",
              "          -1.8427e-01, -9.9401e-02,  1.5673e-01, -8.7944e-02,  5.4096e-01,\n",
              "           8.2155e-01,  4.9568e-01,  4.6473e-01,  8.7235e-02,  2.3082e-01,\n",
              "           9.5084e-02,  4.0983e-01],\n",
              "         [-4.6785e-01,  2.2455e-01,  3.3504e-01, -2.6080e-01, -1.6935e-01,\n",
              "          -1.8976e-01,  3.1253e-01, -1.0232e-01,  3.9499e-01, -4.5061e-01,\n",
              "          -2.6186e-01, -3.7280e-01,  2.8417e-01,  1.7135e-01,  2.4059e-01,\n",
              "           1.3547e-01, -4.6263e-01,  1.3680e-01,  1.6879e-01, -1.6073e-01,\n",
              "          -6.4452e-01,  2.0073e-01,  4.7250e-01,  3.9405e-01,  6.0603e-01,\n",
              "           5.9829e-01,  4.7323e-01, -4.8124e-02,  4.6128e-02,  4.3040e-01,\n",
              "           1.7002e-01, -4.5870e-01],\n",
              "         [-2.7047e-01, -4.4579e-01,  3.2079e-01, -1.9595e-01, -6.7219e-01,\n",
              "          -3.1080e-01, -6.7969e-02, -1.3211e-02,  5.1656e-02, -2.7904e-01,\n",
              "           9.5828e-03,  1.4154e-02, -8.6323e-02, -2.0856e-01,  1.8823e-01,\n",
              "          -1.4729e-01, -3.9219e-01,  6.0849e-01, -6.5109e-02, -1.3620e-01,\n",
              "           1.2661e-01, -1.7821e-01,  2.9009e-01,  7.2730e-01,  4.7218e-01,\n",
              "           1.3158e-01, -2.1474e-01,  9.1919e-01, -4.9524e-02,  1.1799e+00,\n",
              "           1.4476e-01,  3.3542e-03],\n",
              "         [-8.5614e-01, -1.4641e-01, -2.0196e-01, -2.5360e-01, -1.5084e-01,\n",
              "           6.6903e-01,  1.7297e-01,  1.6992e-01,  5.7687e-01, -1.3708e-01,\n",
              "           2.4496e-01,  1.9950e-01,  4.1959e-02,  2.2911e-01, -1.8045e-01,\n",
              "           9.5442e-02,  4.6951e-02, -5.3943e-01, -3.3638e-01, -3.8072e-01,\n",
              "           2.6034e-01, -1.1895e-01, -2.1833e-01, -2.2981e-01,  3.0368e-01,\n",
              "           2.9670e-01,  1.6413e-01,  2.9658e-01,  5.1782e-01, -5.4962e-01,\n",
              "           9.6833e-02,  3.3938e-01],\n",
              "         [-8.5209e-02,  2.6018e-01, -3.9908e-02,  1.6563e-01,  4.1980e-01,\n",
              "          -6.1646e-02, -1.4834e-01,  1.1912e+00,  5.8833e-02, -1.1715e-01,\n",
              "           3.4037e-01, -7.4699e-02,  5.3859e-03,  6.7601e-01,  2.4489e-01,\n",
              "          -3.8835e-01,  5.0900e-02, -2.1787e-01,  2.1688e-01,  1.0255e-01,\n",
              "           2.0122e-01,  3.5796e-01, -2.6046e-01,  8.6226e-03, -1.2784e-01,\n",
              "           5.2525e-01, -2.5477e-01, -2.1504e-01,  2.0857e-01, -4.9431e-01,\n",
              "          -1.3186e-01,  1.3020e-01],\n",
              "         [ 2.6610e-01, -1.2498e-01, -8.3409e-01,  4.2643e-02,  2.7160e-01,\n",
              "          -2.1204e-01, -6.1489e-02,  2.8470e-01, -3.9008e-01, -1.5245e-01,\n",
              "           9.9160e-02, -3.6351e-02, -3.0382e-01,  5.1577e-02, -5.4335e-01,\n",
              "           2.1353e-01,  2.6471e-01, -2.7719e-01, -1.8466e-01, -2.7281e-02,\n",
              "           5.0986e-01, -1.1024e-01,  1.1950e-01, -7.3659e-02, -1.3418e-01,\n",
              "           2.0666e-01,  1.1656e-01, -2.9811e-01, -1.7065e-01,  5.3460e-03,\n",
              "           5.9044e-01,  2.0246e-01]], device='cuda:0', requires_grad=True),\n",
              " 'wHiddenBias': tensor([-0.0482, -0.1806, -0.4369, -0.2504,  0.3192,  0.0583, -0.1260,  0.3432,\n",
              "         -0.0282, -0.6106, -0.4947, -0.3063, -0.7472,  0.3654,  0.4714,  0.2522,\n",
              "          0.6602,  0.4043, -0.2254,  0.2831,  0.3178, -0.5218,  0.3936,  0.9462,\n",
              "          0.1323,  0.3009,  0.5969, -0.2356, -0.5617,  0.6087,  0.6572, -0.2283],\n",
              "        device='cuda:0', requires_grad=True),\n",
              " 'wNeu': tensor([[-2.3964e-03],\n",
              "         [-5.7807e-04],\n",
              "         [-1.4974e-01],\n",
              "         [ 5.7798e-01],\n",
              "         [ 2.5922e-03],\n",
              "         [ 1.8024e-02],\n",
              "         [-8.0977e-02],\n",
              "         [ 8.4696e-01],\n",
              "         [-5.1002e-01],\n",
              "         [-7.0550e-03],\n",
              "         [-3.1381e-03],\n",
              "         [ 1.9088e-03],\n",
              "         [-7.3061e-02],\n",
              "         [ 2.8523e-01],\n",
              "         [ 7.0281e-03],\n",
              "         [ 5.2461e-01],\n",
              "         [-7.2830e-04],\n",
              "         [-6.8295e-01],\n",
              "         [-2.0468e-03],\n",
              "         [ 8.4792e-03],\n",
              "         [ 1.1507e+00],\n",
              "         [ 1.0780e+00],\n",
              "         [-4.9465e-02],\n",
              "         [-5.6872e-01],\n",
              "         [-1.4272e+00],\n",
              "         [-6.3203e-01],\n",
              "         [-5.5071e-01],\n",
              "         [-6.6265e-01],\n",
              "         [-1.0111e-01],\n",
              "         [-1.6836e+00],\n",
              "         [-4.1036e-01],\n",
              "         [ 5.7308e-03]], device='cuda:0', requires_grad=True),\n",
              " 'wNeuBias': tensor([-0.3167], device='cuda:0', requires_grad=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsETFxi2GiBa"
      },
      "source": [
        "torch.save(model,'/content/drive/My Drive/Deepbind_Pytorch.pth')"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}